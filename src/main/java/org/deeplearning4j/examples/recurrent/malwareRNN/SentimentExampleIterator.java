package org.deeplearning4j.examples.recurrent.malwareRNN;

import org.apache.commons.io.FileUtils;
import org.apache.commons.io.FilenameUtils;
import org.deeplearning4j.datasets.iterator.DataSetIterator;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
import org.deeplearning4j.text.tokenization.tokenizer.preprocessor.CommonPreprocessor;
import org.deeplearning4j.text.tokenization.tokenizerfactory.DefaultTokenizerFactory;
import org.deeplearning4j.text.tokenization.tokenizerfactory.TokenizerFactory;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.DataSetPreProcessor;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.INDArrayIndex;
import org.nd4j.linalg.indexing.NDArrayIndex;
import java.io.*;
import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.NoSuchElementException;

import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;

/** This is a DataSetIterator that is specialized for the IMDB review dataset used in the Word2VecSentimentRNN example
 * It takes either the train or test set data from this data set, plus a WordVectors object (typically the Google News
 * 300 pretrained vectors from https://code.google.com/p/word2vec/) and generates training data sets.<br>
 * Inputs/features: variable-length time series, where each word (with unknown words removed) is represented by
 * its Word2Vec vector representation.<br>
 * Labels/target: a single class (negative or positive), predicted at the final time step (word) of each review
 *
 * @author Alex Black
 */
public class SentimentExampleIterator implements DataSetIterator {

    private final int batchSize;
    private int cursor = 0;
    private int vectorSize;
    private int numSamples;

    public SentimentExampleIterator(String dataDirectory, int numSamples, int batchSize, boolean train) throws IOException {

        System.out.println("Been called...");
        this.batchSize = batchSize;
/**
        File p = new File(FilenameUtils.concat(dataDirectory, "aclImdb/" + (train ? "train" : "test") + "/pos/") + "/");
        File n = new File(FilenameUtils.concat(dataDirectory, "aclImdb/" + (train ? "train" : "test") + "/neg/") + "/");
        positiveFiles = p.listFiles();
        negativeFiles = n.listFiles();

        //this.wordVectors = wordVectors;
       // this.truncateLength = truncateLength;

        tokenizerFactory = new DefaultTokenizerFactory();
        tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());
        **/
    }

    @Override
    public DataSet next(int num) {
       // if (cursor >= numSamples) throw new NoSuchElementException();
        System.out.println("Stating data import : " + num);
        try{
                System.out.println("Num : " + num);
            return nextDataSet(num);
        }catch(Exception e){
            System.out.println(e);
            throw new RuntimeException(e);

        }
    }

    private DataSet nextDataSet(int num) throws IOException {

        INDArray input = null;
        INDArray labels = null;

/**
 try
 {

 String csvFile = "/home/pete/data/Superbowl/30.csv";
 BufferedReader br = null;
 String line = "";
 String cvsSplitBy = ",";



 //note if you have more than 2147483647 files this won't work
 //this.numSamples = (int)currMinibatchSize;
 //System.out.println("Line count : " + numSamples);
 //Note the order here:
 // dimension 0 = number of examples in minibatch
 // dimension 1 = size of each vector (i.e., number of characters)
 // dimension 2 = length of each time series/example
 input = Nd4j.create(numSamples,32,100);
 labels = Nd4j.create(numSamples,32,100);


 br = new BufferedReader(new FileReader(csvFile));

 while ((line = br.readLine()) != null)
 {
 // use comma as separator
 String[] country = line.split(cvsSplitBy);
 vectorSize = country.length;

 for (int h=1 ; h < vectorSize; h++)
 {
 int numericVal = Integer.valueOf(country[h]);

 input.putScalar(new int[]{cursor,numericVal,h}, 1.0);

 }

 labels.putScalar(new int[]{cursor,Integer.valueOf(country[0]),0}, 1.0);


 System.out.println("Token feature Vector : " + input);

 cursor++;
 }

 }
 catch (Exception e)
 { e.printStackTrace(); }
 //end 'malware' snippet
 **/
        try {

            System.out.println("Loading training data...");
            String csvFile = "CollinsTestingPreProcessed.arff";
            BufferedReader br = null;
            String line = "";
            String cvsSplitBy = ",";
            int attributeCount = 0;
            List<String[]> rowList = new ArrayList<String[]>();
            String[] country = null;


            while ((line = br.readLine()) != null)
            {
                if (line.substring(0, 10).equals("@attribute")) {
                    attributeCount++;
                }

                if (line.substring(0).equals("{")) {
                    line = line.substring(1,line.length()-1);
                    country = line.split(cvsSplitBy);
                    rowList.add(country);
                }

            }
            // use comma as separator

            System.out.println("Read training data...");

            vectorSize = rowList.size();
            System.out.println(vectorSize);

            //Create data for training
            //Here: we have reviews.size() examples of varying lengths
            INDArray features = Nd4j.create(rowList.size());
            labels = Nd4j.create(rowList.size(), 2);    //Two labels: positive or negative
            //Because we are dealing with reviews of different lengths and only one output at the final time step: use padding arrays
            //Mask arrays contain 1 if data is present at that time step for that example, or 0 if data is just padding
            INDArray featuresMask = Nd4j.zeros(rowList.size());
            INDArray labelsMask = Nd4j.zeros(rowList.size());

            int[] temp = new int[2];

            System.out.println("Creating Feature Vectors...");
            for (int i = 1; i < vectorSize; i++)
            {
                String[] typedDeps = rowList.get(i);
                temp[0] = i;

                for( int j=0; j<typedDeps.length; j++ )
                {
                    //sample row: {356 2.057991,527 2.057991,568 2.057991,833 2.057991,964 2.057991,965 2.057991,1076 2.057991,1927 2.057991,2803 1.667388,2804 1.667388,12779 1.667388}

                    String[] token = typedDeps[j].split(" ");
                    System.out.println("AttID : " + token[0] + " --- " + token[1]);


                    /**
                    //this would hold the feature array - only one element at the moment
                   // INDArray vector = wordVectors.getWordVectorMatrix(token);
                    INDArray vector = Nd4j.create(new String[]{token},new int[]{2,2});


                    features.put(new INDArrayIndex[]{NDArrayIndex.point(i), NDArrayIndex.all(), NDArrayIndex.point(j)}, token);

                        temp[1] = j;
                        featuresMask.putScalar(temp, 1.0);  //Word is present (not padding) for this example + time step -> 1.0 in features mask
                    }

                    int idx = (positive[i] ? 0 : 1);
                    int lastIdx = Math.min(tokens.size(),maxLength);
                    labels.putScalar(new int[]{i,idx,lastIdx-1},1.0);   //Set label: [0,1] for negative, [1,0] for positive
                    labelsMask.putScalar(new int[]{i,lastIdx-1},1.0);   //Specify that an output exists at the final time step for this example
                    **/


                }

            System.out.println("Token feature Vector : " + input);

            cursor++;
            }
        }
        catch (Exception e)
        { e.printStackTrace(); }

/**
        int[] temp = new int[2];
        for( int i=0; i<reviews.size(); i++ ){
            List<String> tokens = allTokens.get(i);
            temp[0] = i;
            //Get word vectors for each word in review, and put them in the training data
            for( int j=0; j<tokens.size() && j<maxLength; j++ ) {
                String token = tokens.get(j);
                if (i == 0) {
                    System.out.println("Token : " + token);
                }
                INDArray vector = wordVectors.getWordVectorMatrix(token);
                if (i == 0) {
                    System.out.println("Token feature Vector : " + vector);
                }
                features.put(new INDArrayIndex[]{NDArrayIndex.point(i), NDArrayIndex.all(), NDArrayIndex.point(j)}, vector);

                temp[1] = j;
                featuresMask.putScalar(temp, 1.0);  //Word is present (not padding) for this example + time step -> 1.0 in features mask
            }

            int idx = (positive[i] ? 0 : 1);
            int lastIdx = Math.min(tokens.size(),maxLength);
            labels.putScalar(new int[]{i,idx,lastIdx-1},1.0);   //Set label: [0,1] for negative, [1,0] for positive
            labelsMask.putScalar(new int[]{i,lastIdx-1},1.0);   //Specify that an output exists at the final time step for this example
        }

       return new DataSet(features,labels,featuresMask,labelsMask);
**/

        return new DataSet(input,labels);
    }


    @Override
    public int totalExamples() {
        return numSamples;
    }

    @Override
    public int inputColumns() {
        return vectorSize;
    }

    @Override
    public int totalOutcomes() {
        return 2;
    }

    @Override
    public void reset() {
        cursor = 0;
    }

    @Override
    public int batch() {
        return batchSize;
    }

    @Override
    public int cursor() {
        return cursor;
    }

    @Override
    public int numExamples() {
        return totalExamples();
    }

    @Override
    public void setPreProcessor(DataSetPreProcessor preProcessor) {
        throw new UnsupportedOperationException();
    }

    @Override
    public List<String> getLabels() {
        return Arrays.asList("positive","negative");
    }

    @Override
    public boolean hasNext() {
        return cursor < numExamples();
    }

    @Override
    public DataSet next() {
        return next(batchSize);
    }

    @Override
    public void remove() {

    }

}
