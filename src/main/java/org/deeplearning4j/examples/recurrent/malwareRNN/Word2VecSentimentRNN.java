package org.deeplearning4j.examples.recurrent.malwareRNN;

import org.apache.commons.compress.archivers.tar.TarArchiveEntry;
import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;
import org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;
import org.apache.commons.io.FileUtils;
import org.apache.commons.io.FilenameUtils;
import org.deeplearning4j.datasets.iterator.AsyncDataSetIterator;
import org.deeplearning4j.datasets.iterator.DataSetIterator;
import org.deeplearning4j.examples.recurrent.word2vecsentiment.*;
import org.deeplearning4j.examples.recurrent.malwareRNN.SentimentExampleIterator;
import org.deeplearning4j.models.embeddings.loader.WordVectorSerializer;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
import org.deeplearning4j.nn.conf.layers.DenseLayer;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.INDArrayIndex;
import org.nd4j.linalg.indexing.NDArrayIndex;
import org.canova.api.records.reader.RecordReader;
import org.canova.api.records.reader.impl.CSVRecordReader;
import org.canova.api.split.FileSplit;
import org.springframework.core.io.ClassPathResource;
import org.deeplearning4j.datasets.canova.RecordReaderDataSetIterator;
import org.deeplearning4j.nn.api.OptimizationAlgorithm;
import org.deeplearning4j.nn.conf.GradientNormalization;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.Updater;
import org.deeplearning4j.nn.conf.layers.GravesLSTM;
import org.deeplearning4j.nn.conf.layers.RnnOutputLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.deeplearning4j.eval.Evaluation;
import org.nd4j.linalg.dataset.SplitTestAndTrain;
import org.nd4j.linalg.lossfunctions.LossFunctions;

import java.io.*;
import java.net.URL;
import java.util.ArrayList;
import java.util.List;

public class Word2VecSentimentRNN {

    public static final String DATA_PATH = "/home/pete/data/Superbowl/30.csv";

    public static void main(String[] args) throws Exception {

        int batchSize = 50;     //Number of examples in each minibatch
        int vectorSize = 10;   //Size of the word vectors. 300 in the Google News model
        int nEpochs = 5;        //Number of epochs (full passes of training data) to train on
        int truncateReviewsToLength = 300;  //Truncate reviews with length (# words) greater than this
        int seed = 123;

        //Set up network configuration
        MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
            .seed(123)
            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1)
            .updater(Updater.RMSPROP)
            .regularization(true).l2(1e-5)
            .weightInit(WeightInit.XAVIER)
            .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue).gradientNormalizationThreshold(1.0)
            //.learningRate(0.0018)
            .learningRate(0.0000001)
            .list(2)
            .layer(0, new GravesLSTM.Builder().nIn(1).nOut(3)
                .activation("softsign").build())
            .layer(1, new RnnOutputLayer.Builder().activation("softmax")
                .lossFunction(LossFunctions.LossFunction.MCXENT).nIn(3).nOut(2).build())
            .pretrain(false).backprop(true).build();



        MultiLayerNetwork net = new MultiLayerNetwork(conf);
        net.init();
        net.setListeners(new ScoreIterationListener(1));


        //DataSetIterators for training and testing respectively
        //Using AsyncDataSetIterator to do data loading in a separate thread; this may improve performance vs. waiting for data to load
//        WordVectors wordVectors = WordVectorSerializer.loadGoogleModel(new File(WORD_VECTORS_PATH), true, false);

        System.out.println("Calling data import...");

        DataSet allData = loadFeatures();
        allData.shuffle();

        SplitTestAndTrain testAndTrain = allData.splitTestAndTrain(0.65);  //Use 65% of data for training

        DataSet trainingData = testAndTrain.getTrain();
        System.out.println("Training Dataset Size = " + trainingData.numExamples());

        //net.fit(trainingData);
        System.out.println("Model trained...");


        for (int i = 0; i < nEpochs; i++)
        {
            net.fit(trainingData);
            //train.reset();
            System.out.println("Epoch " + i + " complete. Starting evaluation:");

            Evaluation eval = new Evaluation();

            DataSet test = testAndTrain.getTest();
            System.out.println("Test Dataset Size = " + test.numExamples());

            INDArray features = test.getFeatureMatrix();
            INDArray lables = test.getLabels();
            INDArray predicted = net.output(features, false);

            //System.out.println(features);

            eval.evalTimeSeries(lables, predicted);

            System.out.println(eval.stats());
            System.out.println(eval.getConfusionMatrix());
        }

        System.out.println("----- Example complete -----");
    }

        public static DataSet loadFeatures() throws IOException
        {
            int vectorSize;
            INDArray labels = null;
            int maxLength=10;

            //try {

            System.out.println("Loading training data...");
            String csvFile = "/home/pete/Downloads/CollinsTestingPreProcessed.arff";
            BufferedReader br = new BufferedReader(new FileReader(csvFile));
            String line = "";
            String cvsSplitBy = ",";
            int attributeCount = 0;
            List<String[]> rowList = new ArrayList<String[]>();
            String[] country = null;


            while ((line = br.readLine()) != null)
            {

                if  (line.length() > 5 ) {
                    if (line.substring(0, 10).equals("@attribute")) {
                        attributeCount++;
                    }

                    if (line.substring(0,1).equals("{")) {
                        line = line.substring(1, line.length() - 1);
                        country = line.split(cvsSplitBy);
                        rowList.add(country);
                    }
                }

            }
            // use comma as separator

            System.out.println("Read training data...");

            vectorSize = rowList.size();
            System.out.println("Number of instances : " + vectorSize);

            INDArray features = Nd4j.create(rowList.size(),1,maxLength);
            labels = Nd4j.create(rowList.size(), 2, maxLength);    //Two labels: positive or negative
            //Because we are dealing with reviews of different lengths and only one output at the final time step: use padding arrays
            //Mask arrays contain 1 if data is present at that time step for that example, or 0 if data is just padding
            INDArray featuresMask = Nd4j.zeros(rowList.size());
            INDArray labelsMask = Nd4j.zeros(rowList.size());

            int[] temp = new int[2];

            System.out.println("Creating Feature Vectors...");

            boolean firstInstance = true;
                //vectorSize
            for (int i = 0; i < vectorSize; i++)
            {
                String[] typedDeps = rowList.get(i);
                temp[0] = i;
                int label = 0;

                for( int j=0; j<typedDeps.length && j<maxLength; j++ )
                {
                    //sample row: {356 2.057991,527 2.057991,568 2.057991,833 2.057991,964 2.057991,965 2.057991,1076 2.057991,1927 2.057991,2803 1.667388,2804 1.667388,12779 1.667388}

                    String[] token = typedDeps[j].split(" ");

                    if (j==0 && token[1].equalsIgnoreCase("Yes"))
                    {
                        label = 1;
                    } else {

                        INDArray nd = Nd4j.create(new float[]{Float.valueOf(token[0])}, new int[]{1, 1});
                        features.put(new INDArrayIndex[]{NDArrayIndex.point(i), NDArrayIndex.all(), NDArrayIndex.point(j)}, nd);
                        temp[1] = j;
                        //featuresMask.putScalar(temp, 1);  //Word is present (not padding) for this example + time step -> 1.0 in features mask
                    }
                }

                int lastIdx = Math.min(typedDeps.length,maxLength);
                labels.putScalar(new int[]{i,label,lastIdx-1},label);
            }

            return new DataSet(features,labels);
}
    private static final int BUFFER_SIZE = 4096;
    private static void extractTarGz(String filePath, String outputPath) throws IOException {
        int fileCount = 0;
        int dirCount = 0;
        System.out.print("Extracting files");
        try(TarArchiveInputStream tais = new TarArchiveInputStream(
                new GzipCompressorInputStream( new BufferedInputStream( new FileInputStream(filePath))))){
            TarArchiveEntry entry;

            /** Read the tar entries using the getNextEntry method **/
            while ((entry = (TarArchiveEntry) tais.getNextEntry()) != null) {
                //System.out.println("Extracting file: " + entry.getName());

                //Create directories as required
                if (entry.isDirectory()) {
                    new File(outputPath + entry.getName()).mkdirs();
                    dirCount++;
                }else {
                    int count;
                    byte data[] = new byte[BUFFER_SIZE];

                    FileOutputStream fos = new FileOutputStream(outputPath + entry.getName());
                    BufferedOutputStream dest = new BufferedOutputStream(fos,BUFFER_SIZE);
                    while ((count = tais.read(data, 0, BUFFER_SIZE)) != -1) {
                        dest.write(data, 0, count);
                    }
                    dest.close();
                    fileCount++;
                }
                if(fileCount % 1000 == 0) System.out.print(".");
            }
        }

        System.out.println("\n" + fileCount + " files and " + dirCount + " directories extracted to: " + outputPath);
    }
}
